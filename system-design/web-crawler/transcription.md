Alright, so the week three of interview preparation, the system design interview. So for today I will need to system design the web crawler. So the web crawler is an internet bot that is systematically like crawl the world wide web and for content and like storing this content somewhere. So it did start like operating with some of like the pool of the seats of URLs and then processing like these Ct URLs just going and crawling like deeper and deeper through the whole internet and so the main goal is that it need to crawl every web page and store this data like the data need to be can be used for different purposes like for example for LLM training or for web search or like different purposes. Alright so this is the system I need to design today. So following the standard framework so I will start with the requirements and I will start with the functional requirements. So the functional requirements for this web crawler, so what what do I need to do? So first of all, as a functional requirement, so this web crawler should crawl the full, crawl the full web starting from the seed. URLs. So we will provide someone will provide to the web crawler. So the next one is what we need to do. So we don't need just to crawl. So we need to extract the text data and store this data. So the next functional requirements is extract text data and store this data. This actually can be not only the text data, this can be like the images or maybe even video, depends on the actually constraints that that interviewer will put. So I will assume, is it like the training? I will assume that we will be limited only with the text data. And yeah, so these like two main functional requirements. So together with the functional requirements, we have like non-functional. And as an unfunctional requirements, what we need, what our system should be. So I will put here that system should be fault tolerant. This means that if during the crawling, if anything will be, will go wrong. So we just will not lose any data. so and they can proceed from the exact same place. So we shouldn't like start every time from the beginning if something is broken. Not losing any data. So the next one is the politeness. So what this means, so the web crawler is like reading the data from the websites and we shouldn't initiate kind of DDoS attack for any site in the web and request this data not harming the existing websites. So we can touch this closely at the design stage. I put it here like for the moment is the politeness requirement. What else? So we need to scale, so it should be scalable. And it's for my knowledge, so the latest data I have the overall number of the web pages websites actually in the internet is kind of like round 1.5, 1.7 billion. So together, like it's with the webpages, like can be probably a couple of times more. So let's put here like five billion webpages. So it should be scalable to this value. Scale to five billion webpages. So it should be efficient. So we don't need to wait like for like years while it will scroll everything. So we will need like to wait. Let's like limit with the one day. crawl, efficient crawl under one day. So this will be our non-functional requirements. We can put here like additional requirement SD, For example, like it can crawl by scale. Like let me put it this functional requirement, like crawl by scheduler. Okay, so these are functional requirements. So now we need to understand what is the scale need to make some like estimations and understand like how our system will look like in terms of the size. So the scale of the system will be defined like by a couple of parameters. So first of all we defined that the five billion web pages are total in internet. Of course it's roughly like the average web page against all permynolage and it's like around like five two megabytes two megabytes per web page so we need to store this data so this is really critical for us so we have one day to crawl everything and so we need to understand do we have this resource limited or unlimited for our case and I assume that interviewer will like tell me that there is unlimited resources. We have. Okay, so this is our scale. Now we can make an estimations. So the estimations we need to store all this data somewhere so let's calculate how much storage space. So we have like 5 billion of web pages so we will multiply it for 2 megabytes for a single page. So what we'll have is kind of back to 5 billion, 5 billion by 2 megabytes. So we will have like 10 billion megabytes. This will be 10 million gigabytes. It will be kind of 10 petabytes. So the 10 petabytes of the space. So other one, other estimation is what is the bandwidth do we have? So we have like one day to crawl and we have like five billion of web pages. So five billion we need to divide by, so in one day we have like 24,000, 3,600 seconds. Es nevarētu, ka mēs nevarēt, bet nevarēt, ka esmu nevarēt. Tāpāk, ko ir 5,5 web pages per second and these 60 web pages so we just need to multiply everything by two megabytes so these will be 60,000 pages multiplied by two megabytes it will will be like roughly 100 of gigabytes of data per second. It will be 100 gigabytes per second. And so this will be like round, so 800 gigabytes per second. So what this number is showing us, so the 100 gigabytes per second is really, really high. So the AWS of the, one of the highest like throughput it's around like 400... 400 megabits per second so we can define how many instances do we need to support these 800 gigabytes per second. So we can like divide this 800 gigabytes per second we need to divide for like let's take 400 Mbps. So what we will have in this case, it will be 2000 machines. So it's roughly, we will need like 2000 machines to crawl everything in one day. Alright so we have these estimation, so let's proceed and let's define our interface. the interface of the crawler. So as an interface we have just simply input and we have an output. So as an input we have the URLs seeds. So this will be like a set of the URLs that we will provide to our system this will be input and we have an output and output will be stored data the data that we will store in our system so the text the website the website text. So now let's define the data flow and we can proceed with this data flow. With the data flow we can proceed with our design. So the data flow will be the following. So we need to take a seed URLs from the some place that it probably will be like the queue and let's call it from the frontier, frontier queue. So then we need to fetch HTML. So we're taking these URLs, then we're fetching HTML. Next we need to extract text from the HTML and store text into storage. Okay, so then we need to extract, we can actually combine this extract text from HTML text and URLs on the HTML store text into the storage and put extracted URLs to the frontier queue. And then we need to repeat everything, starting from the first stage. So this is kind of like the general idea and this unblocking me from the proceeding to the next stage so when I just can start like this high level design, putting some really So let's begin. First of all, we need to have this frontier queue. So I am adding this element, that frontier, frontier queue. So we'll define later what is the technology. We will use for this frontier queue, so this can be like multiple technologies, can be, for example, it was SQS, it can be, I don't know, like even RADIS, or it can be Kafka. They probably more appropriate choose for these requirements and for the system size, or any other, but let's not focus on this for the moment. So this will be a queue. So then we have kind of worker. This worker is the main responsibility, crowder worker, I just called crowder worker. So the main responsibility of this crowder worker is like pool, URL from Frontier. Next is fetch HTML. Then it will be extract text and it will be Extract URLs, extract URLs. All right, so when the scroller like extract the text, extract URLs need to store this data in our storage. I am adding here like additional storage. So the frontier to the crawler, and then from the crawler, crawler is just storing data into our storage. And the worker as well. Moving this some URLs to the frontier queue. I'm first using this tool, I'm drawing tool, so yeah. Making some, give me a second. Yeah, so moving the, so it is like put URLs to the queue. All right, so and here we will like save text to storage. Like this crawler worker, it need to like communicate with some external services, need to communicate with DNS service. So once you like get like HTML, need to go to DNS service and like to get the, the IP address and then using this IP address it need to go to external service that is actually the web pages. Okay. So it will like, and this IP address go to the web pages and like extract this information. So now we have like the (clears throat) kind of the really simple and like in some sense working system, like working, I mean, like conceptually working. All right, so now we can like proceed and address all our like functional requirements. So first of all, let's take a look on our functional requirements. So first functional requirement is that we need to have the seed URLs and we need to extract. Actually, we need to have the seed URLs and we need to start the crawl using the seed URL. So we have this frontier queue with the seed URLs and the crawler is like just using them, crawling everything. So we need to extract the text data in store. So this what exactly this crawler is doing so the URLs fetching HTML, extracting text, save text and extract URLs and fill our frontier queue with the URLs. So what the crowd by scaler. So this one thing that is missing here in this picture. So let's change it. So first of all, I will put some service at the front of here, so I will put the scheduler. That will schedule the... The crawling process. So this scheduler, what it will do. I think that we need to have some, first of all, we need to have some seeds storage. So this storage that will be used for like storing initial seeds. So the scheduler will take these seeds. And scheduler can fill this frontier queue with seeds and it will be doing like, at the beginning it will use these seeds storage. Then we will do the first loop and crawl all the world wide web. So we will have other storage and this scheduler can just actually go to this storage with all the URLs and then it will be used. and put this data to our frontier queue, and then the process will be repeated. Alright, so let's take a look on this storage a little bit. So when the crawler is extracting our worker, extracting the data, the text and URLs, so it will be stored like in the storage, the text itself, and it probably need to be stored additionally, the information about the URLs themselves. So we will store here like the separately in separate storage. So we will store the URL metadata. So we will need information about the, so the URL itself. Ми треба доставити інформацію, коли це була елит. Тиме стиму. Ми треба інформацію. Ми треба, звісно, якісь таки ID. i vidimo, da vidimo da se nek So this storage actually, it can be, so the sizes will not be really big, so it's like 1.5. It's like, as we defined, like five billions max, so with the web pages. So yeah, it can be like DynamoDB, it can be PostgreSQL, so it's like, not really important for the moment. So this storage that will store the text itself, so this storage will be kind of the block storage. We can use kind of what we can use. So we can use for these, for example, the S3 to store the HTML data. All right. So, yeah. So now we have the storage. So let's take a look on our functional requirements. So crawl by scheduler. So we do have this scheduler. We have the scheduler at the beginning. We will take a look on seats, like move these seats to the frontier queue. So the queue, it's probably like at the beginning, we will have like a couple of millions of the URLs, seed URLs. And then like scheduler will, if it's, as we need like to like crawl this every day. So every day scheduler will go to the, to our, after the initial run. So we'll go to our like saved URL storage and just we'll go through all these URLs and then just put them into our Frontier queue and like we will crawl every day. So we just need to update this information to keep it relevant because like the websites they are like changing. Yeah, we need to address a couple of the issues, for example, so we don't need to duplicate information. We don't need to store the same information a couple of times, so then probably this crawler need to be a little bit more smarter. It need to have kind of like calculate maybe hash for the web page, and then like if hash is changed, So to store this data into our storage. All right. So yes, let's take a look closer on the our non-functional requirements. So default tolerant, what does this mean? So default tolerant here means that this crawler worker that we have like actually the distributed system, these like thousands of the worker servers, and these services actually need to be even like, distributed across the geographical regions. So because the crawling speed is actually really important for us, so because we have really big amount of data that need to be crawled. So if our crawlers will be on the single region and it will go to really region it is located far from this exact data center. So this will add latency on the networking just because of the physics. So this is why this system need to be kind of distributed system across the world. So the DNS of the first like fault tolerant, if something will broken in our system, so we shouldn't like lose the data and ideally it should recover as soon as possible. So let's take a look here. So the crawler is going to the DNS for the data. And yeah, so we, oops, oops, oops. Yeah, it's going to the DNS and the DNS here. If we will use only a single service, single DNS, so this actually not, if it will, we'll, we'll down, so our system will down as well. So then probably it's good practice to use like different DNS service and maybe just like, or use them with roundthrowing approach or something like this. So we will like work with the, like we will ask different DNS services to provide IP addresses for our search. So the, let's assume that we, during the, that during the, our worker, work when the worker is just pull the, like pull the URL from the queue and start like, fetched HTML and then like it's like will be the set of the tasks that need to do so that need to like to fetch extract the text, extract the URLs, et cetera. And if something is happening, it's just falling down. This means that these, we will lose the data or we need just to start like from the beginning. So it's probably need good practice B to separate. So to move some of the responsibilities from the worker to other place. And what kind of these responsibilities can be. So we can, for example, leave for the crawler only like some responsibilities is slightly pool URL fetch HTML. it will store the HTML data and it will store URL data, but the extracting and actually analysis of the, this extraction will be in the different service. So we like put here like different like extractor, extracting service, call it like in this way. and this extraction service, what it will do. So this extraction or extraction, extraction service, it will do the following. So it will analyze HTML, extract data. It will can extract the text, it can extract the URLs and even it can extract other data. For example, we will need to extend this platform and like not store only the text, but store for example the images. So we will just change this extraction service to extract other data and store it into the S3 bucket. So this separate extraction service, I will put it right for the moment as just the service that will be having the comments from the our crawler worker. It's probably can be a combination of other queue that is feeding by our crawler worker and the service itself that will calculate the, not calculate, but analyze and extract the data. Like right now it's just the single one. Yeah, so this extraction service will put separate information as the text data to our blob service. block storage, sorry. So the S3 will store HTML data and text data that will be stored by our extraction service. All right, so like feed these URLs, feed this URL. So our crawler will send to this extraction service just the consent just in URL, because the data is the URL we already like store and like the HTML will register in our like storage. So this is extraction service. We'll take this URL, we'll go to the storage, F3 storage. We'll read by this URL the HTML, we'll process this HTML and put this data to the, put this data to the, back to storage. So this will be, it will put like the text data there. Other data that will be processed and put to the frontier queue. So we will have like extract the URLs. And so we, the service like extraction service need to put this URL back to frontier queue. So this will allow us to continuously crawl through all the worldwide web. So the logic inside here, like it's need to be like not really simple logic. So because like we need to understand what we need to understand. So we need to resolve so-called crawling traps. So when we have the infinite links, when we have so-called calendar links, or when we have the links for the pages that are dynamically generate content, And so we need to avoid those traps. So it shouldn't be like, shouldn't go infinitely, crawling like the same, like web site, and like going like in circles, in infinite loops. So this logic need to be inside there, and there are like different approaches how to do this. We will not concentrate for the moment on solving them. And yeah, we will go next, go forward. So what we need to, what we can do further. So let's take a look on our non-functional requirements of the politeness. the politeness is that is how our like, crawler is how our system is actually crawling the webpages. So usually, so there is kind of standard. So we shouldn't like fetch the webpages. the same page like too many times like too frequently it's kind of like one per second is probably will be good enough and it will be pretty much polite so other one approach is that we have like this robot txt that is defined well like which crawlers are allowed or not allowed in our system. So where we can put this logic with this politeness approach. So this politeness approach, as our crawler will work with the FrontierQ, And so we shouldn't put this... Or actually we can put this logic inside the crawler worker. So the... The data, I mean like this... ht... Roboticstdata will be stored together with the URL metadata. So we can... (clears throat) And like when the scheduler will read these URLs from our URL metadata storage and put into the our frontier queue, then crowder like pulling this URL data, can like pull it together with this robot TXT, with this data when it was last visited, et cetera. and just making decision to skip crawling this web page or just to proceed with this web page. So this will be addressing our politeness, non-functional requirement. So the scale up to 5 billion of web pages. So this actually yeah, to do this So we need to first of all, as I mentioned, so we need to have these crowd-workers. They will be multiple machines. So we defined it as like 2,000 of servers. And so they will be distributed across the multiple regions. And actually it is a good approach if some crawlers that are located in the different geographical donts, they will use actually URLs only like located in this geographical donts. So what this means, so this means that we need to store in this URL metadata information about the geolocation. So this how this can be defined, so we can probably define it. (clears throat) So we can define it like (clears throat) So we can define it by like either, by IP address it will be defined like really approximately or it can be defined by like either some like, yeah let's limit with the IP address. So when we will have like IP address from the DNS So we can put here this geolocation, we can convert it into some hash. And then this frontier queue, this will be distributed queue, so we can use Kafka. And it will be like the separate topics for the different geolocations. the different crawlers they will consume from these like geolocation topics. And crawl like only the web pages that are located in their region. So this will actually really speed up the data crawling data reading. So the efficient crawl under one day. So the speed is really important here. And as we can see here, like what our services, we are like external services we are using. So first of all, it's the web pages and we just like address one of the issues with the geolocation. And the second one is like the asking the DNS. So DNS itself can be pretty much pricey in terms of the time. So we probably need to have kind of memory storage. Like let's be ready to sketch. To store the DNS data. So once we will read this DNS data from the DNS service, then we can just put, store it in our Redis cache and then reuse this data for our crawlers. So this will like save across time. Of course, as our system is distributed, this Redis cache need to be distributed as well. And yeah. So, as we have like, 10 more minutes, so we can talk a little bit about the, how we can, what are the bottlenecks at the moment and how we can address these bottlenecks. Yeah, so the, as I mentioned, the frontier queue. So let's take a look like clearly on this frontier queue. So the queue itself, so again, so as I mentioned at the beginning, so we have like a couple of the options which technology we can use for this frontier queue. It was mentioned like that in options, in AWS SQS. I wouldn't go with the SQS here, taking into account our scale. The SQS has and limitations kind of. The problem knowledge is like 3,000 messages per second. There is some limitations on this. On the message size, there is a limitation in terms of the number of the SQSQs for an account and etc etc. So for the large scale it's probably more appropriate will be Kafka it is distributed it is like allowing to support like these throughputs for millions or billions of the messages per second it is persistent and yeah so if if anything will fall, so there is like persistence layer and the data will not be gone anywhere. So using this frontier queue as Kafka queue, we'll address our issue. So inside this Kafka queue, everything, all the URLs, they shouldn't go through the single, through the single queue, through the single topic. So it will be like distributed. As I mentioned, so this probably will be a good approach to use like the separate topics or separate queues inside this Kafka based on the geolocation. Like, so for this case, like this seeds stored in our scheduler, They need to have not just URLs, they need to have like your location data, like converted to probably hash. And the scheduler will like put the URLs to appropriate topic in the queue. What else? So now this Q is the distributed as many topics. So we have the crawler worker that is like distributed system in different locations, different geographical don'ts. So they are used like the separate topics. Okay, so it is like distributed, can handle like the big volume or big size of the data. So the DNS is as well distributed, so the radius cache right now it is like is the single block here but like Redis Cache need to be distributed as well. So distributed again by guio hash, guio by guio location. And the crowder workers in different guio locations will use their Redis Cache. So the web pages is something that is, we just reading here. So let's go further. So the storage. So we have like S3 data storage with text data. So the S3 is actually good blob storage. So it can like store big amount of data. Це дуже виріш like the DynamoDB for example, so it is really scalable, highly available, and it is now SQL, so it's kind of like, can store all this data really efficiently, so it can like, we can read data, again really efficiently based on the different keys, So we have like, it is possible to put there a couple of keys, primary key, secondary key, that are like URL and geolocation. And yeah. All right. looks like that's it. So we have addressed the non-functional requirements. So we have some little bit more time and we can talk a little bit about this extraction service and the logic inside this extraction service. So first of all, we need to understand the the so called crawler trap. Crawler trap logic. So the crawler trap like it can be due to, as I mentioned before, due to separate circumstances. So the web page can be like the calendar web based web page based web page. It's when the content of the web page is defined and generated by the different dates, like in URL, and it can be like infinitely, you know, like you infinitely can scroll to the future. So you will go into this trap. So it can be circle links, circle and trap. So it can be kind of circle dependencies. Dependency strap. So it went the pages, so the page one is heading to the page two, page two heading to page one. So then we will go with these circles infinitely and again, so we will stock in this circle. or else so we can have like this dynamically generated generated content. So that what this means, so that means that the URL itself will define like the page content. I mean, like whatever you will put like in like URL something like this, it will dynamically generate the page content and this will lead to the infinitely like crawling the same web page. What else we can have here as a trap. So this trap is the query parameter. crowd your traps and the extraction service logic need to address all these traps. And we need to probably to limit our crowding or like crowd in depth for single webpage website like in some ways, so it can be like the depths or it can be like the time limitation One other important aspect I didn't touch is that the approach should be a breadth-first search or depth-first search, because the crawling the worldwide web is kind of crawling through the tree, the graph, and you're probably crawling from the top, from the seats, to the all other web pages. So for these crowning, it's probably the better approach. It will be the depth first search. So when you take the URL and then just go to the end of this web pages links there ir jie pras
