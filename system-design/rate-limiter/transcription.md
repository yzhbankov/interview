Hello, so the day two of the system design interview preparation. So today I do have like the task of to system design the rate limiter. And let's begin. So just for to repeat the task itself. So I need to design the rate limiter. So rate limiter will controls how many requests can make within a specific time frame. So it acts like a traffic controller for my API. It is allowed, for example, like 100 requests per minute from a user when rejecting the access requests like some error. So the rate limits prevent abuse, protect your service from being overheld by births of traffic or ensuring the fair usage across all users. So to prevent from the DDoS attacks etc. Alright, so let's put here that we are design the rate limiter. Okay, yeah let me put like one hour in my clock. Okay, yeah the first of all I won't start with so I just need like to understand a bit more. So this is the rate limiter that we will use for some API or it will be used for public API or it will be used for some services inside the system or to be used for anything else. So I assume that it will be like using for for my API. So, okay. Once we identify it, so I need to understand some of the requirements to these rate limiter. So the functional requirements Let's see. So if these service rate limiter will reduce the number of requests from a single user, it looks like it needs to be able to identify user and it can be done by identifying by like different parameters can be identified user by IP address. If this user is not like authenticated it can be identified by API key. If this is user like API user or it can be identified by user ID. ако е върху нонудър, ако е върху системата. the next question is or requirement is probably so it need to be limit the requests following some defined rules some defined rules. Okay. So these other requirements and we like how it need to act. So when service is identified the limit, so it needs to reject. And it's probably need to return proper error with proper error status. This will be the next functional requirement. Okay, so the non-functional requirements. For the non-functional requirements, I need to understand how important for us the availability accessibility consistency, low latency. And let's assume as I train and no interviewer, that for non-functional requirements, we will have like really important availability of the service. and not really important consistency. So this means that even if we will change the rules, so the service will not be interrupted, we will proceed working, but rules probably can propagate not immediately but a little bit later. So it's much better if we will prefer consistency over the availability. This will mean that the, so to immediately apply all the rules and they start working for all the users, it will like, we will sacrifice the availability and service like for some period of time cannot work. So, so I will like put the availability over the consistency. So we put the requirement the low latency rate. Low latency rate limit check. And let's put like less than 10 milliseconds. So just limited with this number. So, and we need to understand, I need to understand like how big is the system, how it can scale. And let's put it like a scalable to one million like requests per second. So we like working really on the catalog. So I do have like these functional requirements. I believe that we probably can proceed with this requirements. I don't need to understand the scale of the system to make any estimations. And I don't need to understand how many users we will have for this service. So I have one number that gets the one million requests per second we need to scale to. So we can set as the like 100 millions of the daily active users. So we, and we have like these one million requests per second. So now having the scale, we can make some estimations. So if the rate limit, the rate limit like should work really fast. So this means that all this information about the users that are requesting our resources and going through the rate limit, they need to be stored somewhere. So if, as we have like the slow latency, less than 10 millisecond, that means that all the information should be keep probably in memory. So we do need to understand how much this memory do we need. And yeah, let's take a look, let's make an estimations. So for the 100 million of the elective users, so this means that we probably need to keep, like we can use this Pareto law and say that like, let's say like 20% of users, they are making like 80% of load on the service. On the one side, from the other side, it's probably we can calculate everything for a peak load when like they can use the service actively and almost at the same time. So let's calculate it for 100 million users. So what data we do we need to keep in memory just to proceed with these calculations to understand like we need to be rejected from the making the API request. So probably we need to keep the user identifier as the user ID or IP address so we can keep it like user ID probably will not be really big. Looks like 100 million daily users so we probably keep like it's UID with two new characters. It will be like more than enough. For IP address it is like it is more than that. So the IP address, let's say we will use IPv6, so it will be 16 bytes. IP is six, that is 16 bytes, so for single user. So this information we need to keep, so we need to keep information about the, so when last request was made, like it's like we need to check the timestamp is the timestamp that is like the four bytes size four bytes. and what else do we need? So the IDINQ file that will be put 16 bytes, the timestamp we will put the four bytes. And yeah, so everything need to be multiplied by 100 of millions. Let's calculate. So it will be like, It will be 20 bytes, hundreds of millions. It will be 20 kilobytes. It will be 20 megabytes. it will be like around the 2 megabyte. say let's calculate it more precisely it will be 100 multiplying by 20 bytes and it's like a kilombytes it's in megabytes and it's in gigabytes it's like around 2GB, it's all on way. So if you like estimation it's not really good. Okay, yeah let's proceed and let's define our core entities. Let's proceed with the design. The core entities for our rate limiter. So we will have the core entity, it is like request. The core entity is like the client with ID, with user, I'm sorry, with IP, with user ID, and maybe with API key. So we do have like the one other rule is the, not the rule, but one other entity, it will be the rule. So for the rate limiting, it can be used like different rules, like the fixed window, sliding window, like the talking bucket, or leaking bucket, different strategies can be used. So let's define the, system interface. So SD is a rate limiter and we need some system interface to maybe define or to read some rules. We need to understand, maybe to check. We need to check the how many requests still have, viewers still have. So let's define the some like internal functions. So this usually can be like HTTP or it can be like other interfaces, IPC, GRPC. So let's suggest we'll concentrate on some functions that is redefined the system interface. So we probably need to have the function is request_allowet and it will like getting like as a client ID to identify it and request ID. So and So it need to return something like s. I love it. Like volume value is like. So we can like return something like remaining. And remaining numbers like how many requests remain for user and we probably can return the reset time. It will be like in time, it will timestamp. Es visu visu visu put some rules for some endpoints we can like do it like through the post request so we can like if we want to make the system like actually flexible and user can just like any internal user can put different user rules, different rate rules for rate limit rules for different endpoints, so we can just put here as an end point into the body of the request, so we can put here, like as the rule, we can have like some rules are defined in our system and then just propagate to our service. So this will allow us to make these requests internally and changing the protections for some endpoints dynamically using the service. Vēl nekā arī Actually, we can replace post with put just to replace and we need to have like this getRequest not for only all the points but we can put here like the endpoint ID. Okay, so we have these core entities. We have the interfaces, so we need to identify where our rate limiter need to be placed. So the rate limiter is the, so we do have like the clients that are using the, so this is section with the high level design HLD. We have a client, we have our service, this can be actually single service or this can be like the microservices. Can assume that it will be, I will assume that it, like the distributed system is many microservices. I put here like the service micro service a, micro service b, and micro service c. So like and we have the API gateway that is routing user requests, to some specific microservice. So when the user is making the request to gateway, gateway decided like where to route these requests to microservice A, to microservice B, or to microservice C. Or yeah, maybe that just will have some complex logic and route to every microservices. So where we can place the, where we can place our rate limiter. So the rate limiter can be placed like in different actually places. So first of all, it can be placed somewhere inside the, the microservice, each of microservice, the meter, like, and like this schema like allow to, so it has like some pros and cons. So the pros will be that the rate limiter is like really close to the application itself. And the time when the request will be handled and identified is that these requests need to be rejected and it will be really minimal. And like, there is no delays or there is like the minimized delays on the requests bringing by the rate limiter. The cons of this approach, they are in the fact that this rate limiter becomes really distributed. It will be like hard to maintain, to synchronize. And yeah, so this kind of, not really good architectural decision. So other place is the API gateway. So placing in API gateway, this will be like not bring a lot of the delay to the user's request. So having this rate limiter is inside the API gateway and protecting our services. So we need to identify how this rate limit the rate limit will work, where we need to store our data for the rate limit. So first of all, the data we need to store in our bucket will depend somehow on the rules we will apply and let's talk a little bit about the rate limiting rules. So we do have the fixed window container rule counter. So what this means? So this means that, for example, for each minute we are limiting some user with the, for example, pre-requests. So this means that if user in minute one made like the two requests, so they will be successful requests, like in minute two, user will make like three requests, let's limit this to, then like the search request will be rejected. And then in the second, in the next minute, so it will be resetted and user can make like this to request the next. So this approach is like really simple, but at the same time, so the simplicity is bringing some corner cases. Let's, the corner case is that the, if we imagine that user can make 100 requests per minute and all these 100 requests user will make at the end of the first minute, and next 100 requests will make like in the first second of the next minute. So totally like it's for two seconds. User will make like 200 requests. So this kind of like corner case that can actually hurt our system. Other technique is the sliding window is when we, when this time window is just like shifting. So the requests make at the beginning. So when the time is lapsing, so the requests at the beginning, they just like removing from this kind of bucket that is accumulating, that is accumulating the request number from the user. So it's time moving, so this time window is just moving forward and the number of requests they will be calculated for some period of time from now to past, like for example, like this, the same one minute and all the accumulated, all the number of the requests make for the last minute from the current time. So this is the sliding window. And this will solve our problem with this corner case. One other is the bucket strategy. So these, so we do have like some buckets. So we, the bucket has some limit of the requests user can do and like making requests if the bucket is like filled then we just start to rejecting the following requests. the bucket has like some tokens, token size, so these token size they can be refilled again based on the time frame. And yeah, so when it will be refilled like the tokens will be removed from the bucket, it will allow to make users like the following requests. So these are like the three different strategies. I will probably go with the simplest one, with the fixed window, to not complicate for the moment. We can unlock it. Okay, let's proceed with the data storage. So the data storage is, so where we are storing our data, where we're storing our information about the requests. And so we, as the latency is critical for us, we need to use something like in-memory and we will use the grids that is in-memory, is supporting in-memory storage. memory storage. So what will store in Redis? What information will store? So we need to store like Kāds, kāds, k So we need to update counters. We need to bracket status that will be stored in. Okay. And yeah, let's talk about the information we will store. So the simplest representation of this information will be something like user ID, it's like Alice information about the, when this request was made from Alice. No, this is not it. So let's concentrate on the, again, so the information we are storing in Alice. So we want to store, So first of all, these rules, so where they can be stored. So the rules can be stored in the file system, so they can be part just of the configuration of the rate limiter, and we don't need to store them in the radius. So in radius, we need to store information about the requests from the users. So we need to store this bucket information. So the bucket information will be related to the user. So information that will be stored in the radius, it will be user identifier. We need to store the bucket itself. like if we are talking about the... So it can be like different users, different policies for these different users. So if we're talking about buckets, so we need like the requests information when the request came in. request time stamp. We need to store additionally to request time stamp the bucket size. We need to store bucket status. That will be calculated like field or not field. Також, ці інформації не багато для нас, для нашого систему. Якщо ми маємо таку бакетність, і коли ми маємо таку бакетність з цієї ревні стимки, ми маємо таку бакетність. So the, and we can just remove the requests. It will be the timestamps, timestamps. And yeah. So again, so the data that we can store in our database is in our radius. So the user identifier, so it can be like IP address or user ID or it can be like API key. It will be stored like information o što se je značit, o što se je izgledat, je izgledat na listu u svim tajem stavu, u svim tajem stavu u svim tajem stavim, the reduce mechanism so we can sd bucket size is the time will elapsed and so we can like just clean up these requests in the bucket for the user and like these will allow user to make the new requests. Okay, the user interface. So when the user will make the requests to the system, so system probably need to respond to the user with some meaningful information. Узор нужно, как ответственность, like to get the first of all, so the error status. When the user will hit the rate limit, so it will be four to nine to many requests. So in the response user will be notified with the Возьмите информацию о рейт-лимите, x-рейт-лимите. Так что вы можете вывести респ ir jis visus jau atveikės. O visą informaciją visus jau visus. Nu, jis kai turėjome, yra rungtynėse. as the system need to be like full tolerant as it's available, we need to keep the... we need to make the charging for the RADIES. So, radius need to be, as a distributed system, on the PEN machines, and provide these, default tolerance. [ Pause ] So when we're talking about the radius component that is... So we can build some radius read replicas. Again, so if the system will be distributed and different API gateways, they will be in the different... в различных местах. Если система будет redistributed через различные геологические зоны, это значит, что мы должны держать эти рег allow our service to keep the low latency on our requests, keep data across different Okay. This is it. (sighs) 
